# Model and Dataset Configuration
model_config:
  base_model: "unsloth/llama-3-8B-Instruct-v0.2" # Using instruct-tuned base model is often better
  dataset_name: "Intel/orca_dpo_pairs"
  output_dir: "llama3_dpo_adapters"

# PEFT (LoRA) Configuration
peft_config:
  r: 16
  lora_alpha: 16
  lora_dropout: 0.05
  bias: "none"
  task_type: "CAUSAL_LM"

# DPO Training Hyperparameters
training_args:
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 32
  gradient_checkpointing: True
  learning_rate: 5.0e-5
  lr_scheduler_type: "cosine"
  max_steps: 100
  logging_steps: 1
  optim: "adamw_torch"
  warmup_steps: 10
  bf16: True # Set to False if your GPU doesn't support bfloat16
  fp16: False
  report_to: "tensorboard"

# Inference Parameters
inference_config:
  temperature: 0.7
  top_p: 0.9
  max_new_tokens: 256
